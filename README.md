## Big Data Specialization - University of California, San Diego

 A repository with course materials, resumes and snippets based on Big Data Specialization at Coursera by University of California, San Diego (UCSD).
 In the section of contents you can find a list of topics to be covered in this repository and by clicking on it you will be redirected to a Markdown file with the real content. Most part of the explanation are from the course material, but some topics include external resources.

Inspired by:
* [awesome-bigdata](https://github.com/onurakpolat/awesome-bigdata)
* [hadoopecosystemtable](http://hadoopecosystemtable.github.io/)


### Course Summary
---
* __Introduction to Big Data__
  1. *Introduction to Big Data*
  2. *Demystifying Data Science*
  3. *Getting Started in Hadoop*

* __Hadoop Platform and Application Framework__
  1. *Big Data Hadoop Stack*
  2. *Hadoop based Applications and Services*
  3. *Hadoop Distributed File System (HDFS)*
  4. *MapReduce*
  5. *Introduction to Apache Spark*

* __Introduction to Big Data Analytics__


### Contents
---
##### Programming Models
  * [MapReduce](#) - *Simplified Data Processing on Large Clusters paper*
  * [Apache Spark](#) - *Framework for writing fast, distributed programs. Use a fast in-memory approach and a clean functional style*

##### Distributed File-System
  * [Apache HDFS 1.x](#) - *Hadoop Distributed File System first generation docs*
  * [Apache HDFS 2.x](#) - *Hadoop Distributed File System second generation docs*

##### No-SQL databases
  * [Apache HBase](#) - *Real-time reading and writing operations in column-oriented large tables*

##### SQL databases
  * [Apache Hive](#) - *Data query and analysis using a SQL-like language*


### Setup a spark developer environment
---
 Tips and automated scripts to install big data tools for developers on your pessoal machine or into a cluster.

##### Local setup:
* [spark-dev-setup](https://github.com/Hguimaraes/spark-dev-setup) - _Repository with scripts and tips to install big data tools (mainly for spark) in a single computer (Windows, Unix and OSX)._

##### Cluster and AWS-EC2 setup:
